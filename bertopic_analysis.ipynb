{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 1: INSTALL & IMPORT (UPDATE)\n",
    "# Install library\n",
    "!pip install -q bertopic pandas requests matplotlib plotly nltk\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# --- PERBAIKAN DISINI ---\n",
    "# Kita download semua resource NLTK yang wajib\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # <--- INI YANG KURANG TADI\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"‚úÖ Setup & Download NLTK Selesai!\")\n",
    "\n",
    "# CELL 2: LOAD, CHUNK, CLEAN & TRAIN\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize # Import di atas biar rapi\n",
    "\n",
    "# Hapus model lama jika ada biar RAM bersih\n",
    "if 'topic_model' in locals(): del topic_model\n",
    "\n",
    "# === KONFIGURASI SUMBER DATA ===\n",
    "source_path = \"https://github.com/rhnrafif/datamining_1/blob/main/data/dataset_pidato_UN.csv\"\n",
    "\n",
    "# === 1. DEFINISI STOPWORDS ===\n",
    "indo_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# Stopwords Custom\n",
    "custom_stopwords = [\n",
    "    'yang', 'di', 'dan', 'ini', 'itu', 'dari', 'ke', 'pada', 'untuk', 'adalah', \n",
    "    'sebagai', 'dengan', 'dalam', 'juga', 'karena', 'bahwa', 'tersebut', 'oleh', \n",
    "    'atau', 'sudah', 'saya', 'kita', 'kami', 'mereka', 'anda', 'dia',\n",
    "    'bapak', 'ibu', 'saudara', 'hadirin', 'sekalian', 'terima', 'kasih', \n",
    "    'assalamualaikum', 'waalaikumsalam', 'warahmatullahi', 'wabarakatuh',\n",
    "    'salam', 'hormat', 'selamat', 'pagi', 'siang', 'sore', 'malam',\n",
    "    'yang', 'mulia', 'para', 'presiden', 'indonesia', 'bangsa', 'negara'\n",
    "]\n",
    "\n",
    "final_stopwords = list(set(indo_stopwords + custom_stopwords))\n",
    "\n",
    "def get_raw_url(github_url):\n",
    "    if 'github.com' in github_url and '/blob/' in github_url:\n",
    "        return github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "    return github_url\n",
    "\n",
    "# --- FUNGSI CHUNKER ---\n",
    "def split_long_text(text):\n",
    "    text = str(text)\n",
    "    text = text.replace('\"\"\"', '\"').replace('\"\"', '\"')\n",
    "    \n",
    "    # 1. Pecah per Enter (Paragraf)\n",
    "    chunks = text.split('\\n')\n",
    "    \n",
    "    # 2. Fallback: Pecah per Kalimat jika tidak ada enter\n",
    "    if len(chunks) < 2:\n",
    "        # Ini akan menggunakan 'punkt_tab' yang baru didownload\n",
    "        chunks = sent_tokenize(text)\n",
    "\n",
    "    clean_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()\n",
    "        if len(chunk) > 30: \n",
    "            clean_chunks.append(chunk)\n",
    "    return clean_chunks\n",
    "\n",
    "try:\n",
    "    print(f\"üîç Memeriksa sumber data: {source_path}\")\n",
    "    \n",
    "    # --- LOAD DATA ---\n",
    "    load_params = {'sep': 'üíæ', 'engine': 'python', 'header': None, 'names': ['text']}\n",
    "\n",
    "    if source_path.startswith('http'):\n",
    "        df_raw = pd.read_csv(get_raw_url(source_path), **load_params)\n",
    "    else:\n",
    "        try:\n",
    "             df_raw = pd.read_csv(source_path, **load_params)\n",
    "        except:\n",
    "             df_raw = pd.read_csv(source_path, on_bad_lines='skip', engine='python', names=['text'])\n",
    "\n",
    "    col_text = df_raw.columns[0]\n",
    "    \n",
    "    # --- PROSES CHUNKING ---\n",
    "    print(\"‚è≥ Sedang memecah pidato panjang...\")\n",
    "    all_paragraphs = []\n",
    "    for full_speech in df_raw[col_text]:\n",
    "        paragraphs = split_long_text(full_speech)\n",
    "        all_paragraphs.extend(paragraphs)\n",
    "    \n",
    "    df_clean = pd.DataFrame(all_paragraphs, columns=['text'])\n",
    "    print(f\"‚úÖ Siap Analisis: {len(df_clean)} Paragraf.\")\n",
    "\n",
    "    # --- TRAINING BERTOPIC ---\n",
    "    vectorizer_model = CountVectorizer(stop_words=final_stopwords)\n",
    "\n",
    "    print(\"\\n‚è≥ Sedang melatih BERTopic...\")\n",
    "    \n",
    "    docs = df_clean['text'].tolist()\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        language=\"multilingual\", \n",
    "        vectorizer_model=vectorizer_model, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    print(\"‚úÖ Training Selesai!\")\n",
    "\n",
    "    # --- HASIL ---\n",
    "    print(\"\\n=== INFO TOPIK ===\")\n",
    "    display(topic_model.get_topic_info().head(10)) \n",
    "\n",
    "    print(\"\\n=== KATA KUNCI UTAMA (Topik 0) ===\")\n",
    "    print(topic_model.get_topic(0))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "if 'topic_model' in locals():\n",
    "    print(\"üìä Visualisasi Data...\")\n",
    "    try:\n",
    "        # Barchart kata kunci\n",
    "        fig1 = topic_model.visualize_barchart(top_n_topics=8)\n",
    "        fig1.show()\n",
    "        \n",
    "        # Peta jarak antar topik (Intertopic Distance)\n",
    "        fig2 = topic_model.visualize_topics()\n",
    "        fig2.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal visualisasi: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
