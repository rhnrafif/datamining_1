{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "!pip install pandas gensim nltk pyLDAvis wordcloud plotly matplotlib requests\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Download resource NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"‚úÖ Library siap digunakan.\")\n",
    "# CELL 2: LOAD DATA & PREPROCESSING\n",
    "if 'df' in locals(): del df\n",
    "\n",
    "# === KONFIGURASI SUMBER DATA ===\n",
    "# Ganti dengan path lokal ATAU URL GitHub Raw\n",
    "# Contoh data kamu: text,label (CSV)\n",
    "# source_path = \"data/dataset_pidato_3k.csv\" \n",
    "source_path = \"https://github.com/rhnrafif/datamining_1/blob/main/data/dataset_pidato_3k.csv\" \n",
    "\n",
    "def get_raw_url(github_url):\n",
    "    if 'github.com' in github_url and '/blob/' in github_url:\n",
    "        return github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "    return github_url\n",
    "\n",
    "# --- STOPWORDS SETUP ---\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "custom_slang = {'yg', 'gak', 'ga', 'kalo', 'kl', 'bgt', 'dr', 'dlm', 'tdk', 'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'ny', 'sy', 'aku', 'saya', 'kamu', 'dia', 'ini', 'itu', 'dan', 'di', 'ke', 'dari', 'yang', 'pada', 'untuk', 'adalah', 'sebagai'}\n",
    "stop_words = stop_words.union(custom_slang)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Hapus tanda baca\n",
    "    text = re.sub(r'\\d+', '', text)     # Hapus angka\n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "try:\n",
    "    print(f\"üîç Memeriksa sumber data: {source_path}\")\n",
    "    \n",
    "    # 1. LOAD DATA\n",
    "    if source_path.startswith('http'):\n",
    "        print(\"üåç URL Terdeteksi. Download data...\")\n",
    "        raw_url = get_raw_url(source_path)\n",
    "        df = pd.read_csv(raw_url)\n",
    "    else:\n",
    "        print(\"üìÇ File Lokal Terdeteksi...\")\n",
    "        if not os.path.exists(source_path):\n",
    "            # Auto-search path\n",
    "            for root, dirs, files in os.walk(\".\"):\n",
    "                if os.path.basename(source_path) in files:\n",
    "                    source_path = os.path.join(root, os.path.basename(source_path))\n",
    "                    print(f\"   ‚ö†Ô∏è Path dikoreksi ke: {source_path}\")\n",
    "                    break\n",
    "        df = pd.read_csv(source_path)\n",
    "\n",
    "    # Auto-detect text column\n",
    "    col_text = next((c for c in df.columns if c.lower() in ['text', 'tweet', 'content', 'review']), df.columns[0])\n",
    "    print(f\"‚úÖ Data Loaded ({len(df)} baris). Kolom Teks: '{col_text}'\")\n",
    "\n",
    "    # 2. PREPROCESSING & TOKENISASI\n",
    "    print(\"‚è≥ Membersihkan teks & Tokenisasi...\")\n",
    "    data_words = df[col_text].apply(clean_text).tolist()\n",
    "\n",
    "    # 3. BUILD BIGRAMS (Opsional tapi Penting)\n",
    "    # Mengubah \"rumah\" + \"sakit\" menjadi \"rumah_sakit\" jika sering muncul bersama\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    data_ready = [bigram_mod[doc] for doc in data_words]\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing Selesai. Contoh data baris pertama:\\n{data_ready[0]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "# CELL 3: TRAINING LDA MODEL\n",
    "try:\n",
    "    # 1. Create Dictionary & Corpus\n",
    "    id2word = corpora.Dictionary(data_ready)\n",
    "    \n",
    "    # Filter kata yang terlalu jarang (muncul di <2 dokumen) atau terlalu umum (muncul di >90% dokumen)\n",
    "    id2word.filter_extremes(no_below=2, no_above=0.9)\n",
    "    \n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "    # 2. Train LDA Model\n",
    "    # Ganti num_topics sesuai kebutuhan (misal 3, 5, atau 10)\n",
    "    NUM_TOPICS = 3 \n",
    "    \n",
    "    print(f\"‚è≥ Sedang melatih LDA dengan {NUM_TOPICS} topik...\")\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=NUM_TOPICS, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "    print(\"‚úÖ Training Selesai!\")\n",
    "    \n",
    "    # 3. Tampilkan Keyword per Topik\n",
    "    print(\"\\n=== KATA KUNCI PER TOPIK ===\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topik {idx}: {topic}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error Training: {e}\")\n",
    "# CELL 4: VISUALISASI INTERAKTIF (pyLDAvis)\n",
    "# Enable notebook mode\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare visualization data\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "print(\"Visualisasi Interaktif (Geser mouse ke bubble untuk melihat detail topik):\")\n",
    "vis\n",
    "# CELL 5: WORD CLOUD & BAR CHART PER TOPIK (FIXED)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FIX: Langsung ambil list warnanya saja\n",
    "cols = plt.get_cmap('tab10').colors \n",
    "\n",
    "# Loop setiap topik\n",
    "for t in range(lda_model.num_topics):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Ambil kata-kata untuk topik ini\n",
    "    # Format lda_model.show_topic: [('kata1', 0.1), ('kata2', 0.05)]\n",
    "    topic_words = dict(lda_model.show_topic(t, 20))\n",
    "    \n",
    "    # --- 1. WORD CLOUD (KIRI) ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    wordcloud = WordCloud(background_color='white', width=800, height=600, max_words=50)\n",
    "    wordcloud.generate_from_frequencies(topic_words)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Word Cloud - Topik {t}', fontsize=16)\n",
    "    \n",
    "    # --- 2. BAR CHART (KANAN) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df_chart = pd.DataFrame(list(topic_words.items()), columns=['Word', 'Weight'])\n",
    "    \n",
    "    # Gunakan warna dari palet tab10 secara bergiliran\n",
    "    color_idx = t % len(cols)\n",
    "    plt.barh(df_chart['Word'], df_chart['Weight'], color=cols[color_idx])\n",
    "    \n",
    "    plt.gca().invert_yaxis() # Kata bobot terbesar di atas\n",
    "    plt.title(f'Kata Kunci Dominan - Topik {t}', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# CELL 6: DOMINANT TOPIC & RELATION WITH LABEL\n",
    "# 1. Cari Topik Dominan untuk setiap dokumen\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row[0], key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                sent_topics_df = pd.concat([sent_topics_df, pd.DataFrame([[int(topic_num), round(prop_topic,4)]])], ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution']\n",
    "    return sent_topics_df\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model, corpus, data_ready)\n",
    "\n",
    "# Gabungkan dengan Dataframe Asli\n",
    "df_dominant = pd.concat([df_topic_sents_keywords, df], axis=1)\n",
    "\n",
    "print(\"\\n=== CONTOH DATA DENGAN PREDIKSI TOPIK ===\")\n",
    "display(df_dominant[['Dominant_Topic', 'Perc_Contribution', col_text]].head())\n",
    "\n",
    "# 2. Visualisasi Hubungan Topik vs Label Manual (Jika ada kolom label)\n",
    "col_label = next((c for c in df.columns if c.lower() in ['label', 'kategori', 'category']), None)\n",
    "\n",
    "if col_label:\n",
    "    print(f\"\\n=== HUBUNGAN TOPIK AI vs LABEL MANUAL '{col_label}' ===\")\n",
    "    \n",
    "    # Buat Crosstab\n",
    "    crosstab = pd.crosstab(df_dominant[col_label], df_dominant['Dominant_Topic'])\n",
    "    \n",
    "    # Plot Heatmap\n",
    "    import seaborn as sns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(crosstab, annot=True, fmt='d', cmap='YlGnBu')\n",
    "    plt.title(f\"Heatmap: Label Asli vs Topik LDA\")\n",
    "    plt.ylabel(\"Label Manual\")\n",
    "    plt.xlabel(\"Topik Hasil LDA\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Insight: Warna gelap menunjukkan konsentrasi dokumen. Jika label manual 'Ekonomi' banyak masuk ke Topik 0, berarti Topik 0 membahas Ekonomi.\")\n",
    "else:\n",
    "    print(\"Kolom label tidak ditemukan untuk perbandingan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
