{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b055066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip install pandas gensim nltk\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Download resource NLTK untuk preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac9c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_url(github_url):\n",
    "    if 'github.com' in github_url and '/blob/' in github_url:\n",
    "        return github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "    return github_url\n",
    "\n",
    "# === CONFIG URL ===\n",
    "url_file_github = \"https://github.com/USERNAME_KAMU/NAMA_REPO/blob/main/data.csv\"\n",
    "\n",
    "try:\n",
    "    print(\"⏳ Loading data...\")\n",
    "    df = pd.read_csv(get_raw_url(url_file_github))\n",
    "    \n",
    "    # Auto-detect text column\n",
    "    col_text = next((c for c in df.columns if c.lower() in ['text', 'tweet', 'content', 'review']), df.columns[0])\n",
    "    print(f\"✅ Data Loaded. Target Column: {col_text}\")\n",
    "\n",
    "    # 1. Preprocessing Sederhana\n",
    "    print(\"⏳ Preprocessing...\")\n",
    "    stop_words = set(stopwords.words('indonesian')) # Ganti 'english' jika datanya inggris\n",
    "    \n",
    "    def clean_text(text):\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        # Hapus angka, tanda baca, dan stopwords\n",
    "        return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    processed_docs = df[col_text].apply(clean_text)\n",
    "\n",
    "    # 2. Buat Dictionary & Corpus\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
    "\n",
    "    # 3. Training LDA\n",
    "    print(\"⏳ Training LDA Model (3 Topik)...\")\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=dictionary,\n",
    "                                               num_topics=3, \n",
    "                                               random_state=100,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha='auto',\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "    # 4. Tampilkan Hasil\n",
    "    print(\"\\n=== HASIL TOPIK LDA ===\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topik {idx}: {topic}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
