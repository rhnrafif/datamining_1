{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 1: IMPORT LIBRARY (BAWAAN COLAB)\n",
    "# Tidak perlu pip install library aneh-aneh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 2: LOAD DATA & PREPROCESSING\n",
    "if 'df' in locals(): del df\n",
    "\n",
    "# === CONFIG URL ===\n",
    "# source_path = \"data/dataset_pidato_3k.csv\"\n",
    "source_path = \"https://github.com/rhnrafif/datamining_1/blob/main/data/dataset_pidato_3k.csv\"\n",
    "\n",
    "def get_raw_url(github_url):\n",
    "    if 'github.com' in github_url and '/blob/' in github_url:\n",
    "        return github_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "    return github_url\n",
    "\n",
    "try:\n",
    "    print(f\"üîç Memeriksa sumber data: {source_path}\")\n",
    "    \n",
    "    # Load Data\n",
    "    if source_path.startswith('http'):\n",
    "        df = pd.read_csv(get_raw_url(source_path))\n",
    "    else:\n",
    "        if not os.path.exists(source_path):\n",
    "             for root, dirs, files in os.walk(\".\"):\n",
    "                if os.path.basename(source_path) in files:\n",
    "                    source_path = os.path.join(root, os.path.basename(source_path))\n",
    "                    break\n",
    "        df = pd.read_csv(source_path)\n",
    "\n",
    "    # Auto-detect text column\n",
    "    col_text = next((c for c in df.columns if c.lower() in ['text', 'tweet', 'content', 'review']), df.columns[0])\n",
    "    print(f\"‚úÖ Data Loaded. Kolom Teks: '{col_text}'\")\n",
    "\n",
    "    # Stopwords sederhana (Manual biar ga perlu download NLTK pun bisa jalan)\n",
    "    stop_words = {'dan', 'di', 'ke', 'dari', 'yang', 'pada', 'untuk', 'adalah', 'sebagai', \n",
    "                  'yg', 'gak', 'ga', 'kalo', 'kl', 'bgt', 'dr', 'dlm', 'tdk', 'jd', 'jgn', \n",
    "                  'sdh', 'aja', 'n', 't', 'ny', 'sy', 'aku', 'saya', 'kamu', 'dia', 'ini', 'itu'}\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) \n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return \" \".join([w for w in text.split() if w not in stop_words and len(w)>2])\n",
    "\n",
    "    df['clean_text'] = df[col_text].apply(clean_text)\n",
    "    print(\"‚úÖ Preprocessing Selesai.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error Load Data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 3: TRAINING MODEL (NMF - Scikit Learn)\n",
    "try:\n",
    "    print(\"‚è≥ Sedang melatih model NMF\")\n",
    "    \n",
    "    # 1. Ubah teks jadi Angka (TF-IDF)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "    \n",
    "    # 2. Jalankan NMF (Mencari pola topik)\n",
    "    NUM_TOPICS = 3\n",
    "    nmf_model = NMF(n_components=NUM_TOPICS, random_state=1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "    \n",
    "    print(\"‚úÖ Training Selesai!\")\n",
    "    \n",
    "    # Fungsi menampilkan topik\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topic_data = {}\n",
    "    for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "        # Ambil 20 kata teratas\n",
    "        top_indices = topic.argsort()[:-21:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        top_weights = [topic[i] for i in top_indices]\n",
    "        topic_data[topic_idx] = dict(zip(top_words, top_weights))\n",
    "        print(f\"Topik {topic_idx}: {', '.join(top_words[:5])}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error Training: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86147f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 4: VISUALISASI (Word Cloud & Bar Chart)\n",
    "# Warna visualisasi\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for t in range(NUM_TOPICS):\n",
    "    if t in topic_data:\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        \n",
    "        # Data untuk chart\n",
    "        words_dict = topic_data[t]\n",
    "        \n",
    "        # 1. Word Cloud\n",
    "        plt.subplot(1, 2, 1)\n",
    "        wordcloud = WordCloud(background_color='white', width=800, height=600).generate_from_frequencies(words_dict)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Word Cloud - Topik {t}', fontsize=16)\n",
    "        \n",
    "        # 2. Bar Chart\n",
    "        plt.subplot(1, 2, 2)\n",
    "        items = list(words_dict.items())[:10] # Top 10 kata\n",
    "        plt.barh([x[0] for x in items], [x[1] for x in items], color=colors[t % len(colors)])\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title(f'Kata Kunci Dominan - Topik {t}', fontsize=16)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3604bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELL 5: PREDIKSI TOPIK PADA DOKUMEN\n",
    "# Prediksi topik untuk setiap dokumen\n",
    "topic_values = nmf_model.transform(tfidf)\n",
    "df['Dominant_Topic'] = topic_values.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== CONTOH HASIL PREDIKSI ===\")\n",
    "display(df[[col_text, 'Dominant_Topic']].head())\n",
    "\n",
    "# Cek distribusi topik\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x='Dominant_Topic', data=df)\n",
    "plt.title('Jumlah Dokumen per Topik')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
